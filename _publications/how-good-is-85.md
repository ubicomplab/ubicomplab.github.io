---
abstract: |-
  Many HCI and ubiquitous computing systems are characterized by two important properties: their output is uncertain-it has an associated accuracy that researchers attempt to optimize-and this uncertainty is user-facing-it directly affects the quality of the user experience. Novel classifiers are typically evaluated using measures like the F1 score-but given an F-score of (e.g.) 0.85, how do we know whether this performance is good enough? Is this level of uncertainty actually tolerable to users of the intended application-and do people weight precision and recall equally? We set out to develop a survey instrument that can systematically answer such questions. We introduce a new measure, acceptability of accuracy, and show how to predict it based on measures of classifier accuracy. Out tool allows us to systematically select an objective function to optimize during classifier evaluation, but can also offer new insights into how to design feedback for user-facing classification systems (e.g., by combining a seemingly-low-performing classifier with appropriate feedback to make a highly usable system). It also reveals potential issues with the ubiquitous F1-measure as applied to user-facing systems.
authors:
- Matthew Kay
- patel
- Julie A. Kientz
award: ''
bibtex: |-
  @inproceedings{Kay:2015:GST:2702123.2702603,
   author = {Kay, Matthew and Patel, Shwetak N. and Kientz, Julie A.},
   title = {How Good is 85%?: A Survey Tool to Connect Classifier Evaluation to Acceptability of Accuracy},
   booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
   series = {CHI '15},
   year = {2015},
   isbn = {978-1-4503-3145-6},
   location = {Seoul, Republic of Korea},
   pages = {347--356},
   numpages = {10},
   url = {http://doi.acm.org/10.1145/2702123.2702603},
   doi = {10.1145/2702123.2702603},
   acmid = {2702603},
   publisher = {ACM},
   address = {New York, NY, USA},
   keywords = {accuracy, accuracy acceptability, classifiers, inference, machine learning, sensors},
  }
caption: ''
citation: |-
  Matthew Kay, Shwetak N. Patel, and Julie A. Kientz. 2015. How Good is 85%?: A Survey Tool to Connect Classifier Evaluation to Acceptability of Accuracy.  In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI '15). ACM, New York, NY, USA,  347-356. DOI: http://dx.doi.org/10.1145/2702123.2702603
conference: Conference on Human Factors in Computing Systems (CHI), 2015
date: '2015-04-18'
image: ''
pdf: /pdfs/how-good-is-85.pdf
thumbnail: '/images/pubs/howgoodis_thumbnail.jpg'
title: 'How Good is 85%?: A Survey Tool to Connect Classifier Evaluation to Acceptability
  of Accuracy'
video: ''
video_embed: ''
---